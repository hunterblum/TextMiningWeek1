{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes on Political Text\n",
    "\n",
    "In this notebook we use Naive Bayes to explore and classify political data. See the `README.md` for full details. You can download the required DB from the shared dropbox or from blackboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import re\n",
    "import collections\n",
    "\n",
    "# Feel free to include your text patterns functions\n",
    "#from text_functions_solutions import clean_tokenize, get_patterns\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import functions that we'll need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Script\n",
    "sw = stopwords.words(\"english\")\n",
    "punctuation = set(punctuation) # speeds up comparison\n",
    "tw_punct = punctuation - {\"#\", \"'\"}\n",
    "\n",
    "def remove_stop(text, sw=sw) :\n",
    "    # modify this function to remove stopwords\n",
    "    return([ch for ch in text if ch not in sw])\n",
    "\n",
    "\n",
    "def remove_punctuation(text, punct_set=tw_punct) : \n",
    "   text = [''.join(ch for ch in word if ch not in punct_set)\n",
    "           for word in text]\n",
    "   \n",
    "   return(text)\n",
    "\n",
    "\n",
    "def tokenize(text) : \n",
    "    \"\"\" Splitting on whitespace rather than the book's tokenize function. That \n",
    "        function will drop tokens like '#hashtag' or '2A', which we need for Twitter. \"\"\"\n",
    "    \n",
    "    # modify this function to return tokens\n",
    "    text = text.strip().split()\n",
    "    return(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "convention_db = sqlite3.connect(\"2020_Conventions.db\")\n",
    "convention_cur = convention_db.cursor()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Exploratory Naive Bayes\n",
    "\n",
    "We'll first build a NB model on the convention data itself, as a way to understand what words distinguish between the two parties. This is analogous to what we did in the \"Comparing Groups\" class work. First, pull in the text \n",
    "for each party and prepare it for use in Naive Bayes.  \n",
    "\n",
    "Note: A bit of reversal from last week's strategy. To deal with stopwords at the end of a sentence (e.g. He tossed it.), I removed punctuation except for apostrophes to save the contractions for stopword removal. This seemed to line up with the assertion a few chunks down. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "convention_data = []\n",
    "\n",
    "# fill this list up with items that are themselves lists. The \n",
    "# first element in the sublist should be the cleaned and tokenized\n",
    "# text in a single string. The second element should be the party. \n",
    "\n",
    "query_results = convention_cur.execute(\n",
    "                            '''\n",
    "                            SELECT text, party\n",
    "                            FROM conventions;\n",
    "                            ''')\n",
    "\n",
    "for row in query_results :\n",
    "\n",
    "    # Run the text variable through cleaning steps\n",
    "    text = row[0]\n",
    "    text_low = text.lower()\n",
    "    text_tok = tokenize(text_low)\n",
    "    text_clean = remove_punctuation(text_tok)\n",
    "    text_stop = remove_stop(text_clean)\n",
    "    text_untok = \" \".join(text_stop)\n",
    "    \n",
    "    # Store party as list\n",
    "    party = row[1]\n",
    "    \n",
    "    \n",
    "    # Create temp list\n",
    "    temp_list = [text_untok, party]\n",
    "    \n",
    "    # Append to convention_data\n",
    "    convention_data.append(temp_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some random entries and see if they look right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['foreign prince', 'Republican'],\n",
       " ['reproductive justice', 'Democratic'],\n",
       " ['mission fight future equal ideals founders hopes children sacrifices veterans brave men women uniform families',\n",
       "  'Democratic'],\n",
       " ['black americans standing native land probably represent oregon dual viruses covid19 racism laid bare equal healthcare access deaths communities color',\n",
       "  'Democratic'],\n",
       " ['joe’s purpose always driven forward strength unstoppable faith unshakable it’s politicians political parties even it’s providence god faith us yes many classrooms quiet right playgrounds still listen closely hear sparks change air across country educators parents first responders americans walks life putting shoulders back fighting haven’t given need leadership worthy nation worthy honest leadership bring us back together recover pandemic prepare whatever else next dr',\n",
       "  'Democratic'],\n",
       " ['he’ll love heart', 'Democratic'],\n",
       " ['rhode island ocean state restaurant fishing industry decimated pandemic lucky governor gina raimondo whose program lets fishermen sell catches directly public state appetizer calamari available 50 states calamari comeback state rhode island casts 1 vote bernie sanders 34 votes next president joe biden',\n",
       "  'Democratic'],\n",
       " ['knows it’s like send child war', 'Democratic'],\n",
       " ['america', 'Democratic'],\n",
       " ['trillions dollars repatriated back united states sitting foreign lands far long america became envy world renewed strength came leverage president demanded allies pay fair share defense western world father rebuilt mighty american military adding new jets aircraft carriers increased wages incredible men women uniform expanded military defense budget 721 billion per year america longer weak eye enemy moment president trump ordered special forces kill deadliest terrorists planet day mighty moab dropped insurgent camps day america took stance never defeated enemy albaghdadi soleimani dead issue issue economy wall military trade deals tax cuts supreme court justices va hospitals prescription drugs school choice right try moving embassy jerusalem peace middle east neverending wars finally ended promises made promises first time kept',\n",
       "  'Republican']]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choices(convention_data,k=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that looks good, we now need to make our function to turn these into features. In my solution, I wanted to keep the number of features reasonable, so I only used words that occur at least `word_cutoff` times. Here's the code to test that if you want it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a word cutoff of 5, we have 2391 as features in the model.\n"
     ]
    }
   ],
   "source": [
    "word_cutoff = 5\n",
    "\n",
    "tokens = [w for t, p in convention_data for w in t.split()]\n",
    "\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "\n",
    "feature_words = set()\n",
    "\n",
    "for word, count in word_dist.items() :\n",
    "    if count > word_cutoff :\n",
    "        feature_words.add(word)\n",
    "        \n",
    "print(f\"With a word cutoff of {word_cutoff}, we have {len(feature_words)} as features in the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_features(text,fw = feature_words) :\n",
    "    \"\"\"Given some text, this returns a dictionary holding the\n",
    "       feature words.\n",
    "       \n",
    "       Args: \n",
    "            * text: a piece of text in a continuous string. Assumes\n",
    "            text has been cleaned and case folded.\n",
    "            * fw: the *feature words* that we're considering. A word \n",
    "            in `text` must be in fw in order to be returned. This \n",
    "            prevents us from considering very rarely occurring words.\n",
    "        \n",
    "       Returns: \n",
    "            A dictionary with the words in `text` that appear in `fw`. \n",
    "            Words are only counted once. \n",
    "            If `text` were \"quick quick brown fox\" and `fw` = {'quick','fox','jumps'},\n",
    "            then this would return a dictionary of \n",
    "            {'quick' : True,\n",
    "             'fox' :    True}\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Your code here\n",
    "\n",
    "    # Args:\n",
    "    text_tok = text.split()\n",
    "    \n",
    "    text_feat_words = [ch for ch in text_tok if ch in feature_words]\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    ret_dict = dict.fromkeys(text_feat_words, True)\n",
    "    \n",
    "    return(ret_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(feature_words)>0)\n",
    "assert(conv_features(\"donald is the president\",feature_words)==\n",
    "       {'donald':True,'president':True})\n",
    "assert(conv_features(\"people are american in america\",feature_words)==\n",
    "                     {'america':True,'american':True,\"people\":True})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build our feature set. Out of curiosity I did a train/test split to see how accurate the classifier was, but we don't strictly need to since this analysis is exploratory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(conv_features(text,feature_words), party) for (text, party) in convention_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20220507)\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "test_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "test_set, train_set = featuresets[:test_size], featuresets[test_size:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   china = True           Republ : Democr =     25.8 : 1.0\n",
      "                   votes = True           Democr : Republ =     23.8 : 1.0\n",
      "             enforcement = True           Republ : Democr =     21.5 : 1.0\n",
      "                 destroy = True           Republ : Democr =     19.2 : 1.0\n",
      "                freedoms = True           Republ : Democr =     18.2 : 1.0\n",
      "                 climate = True           Democr : Republ =     17.8 : 1.0\n",
      "                supports = True           Republ : Democr =     17.1 : 1.0\n",
      "                   crime = True           Republ : Democr =     16.1 : 1.0\n",
      "                   media = True           Republ : Democr =     14.9 : 1.0\n",
      "                 beliefs = True           Republ : Democr =     13.0 : 1.0\n",
      "               countries = True           Republ : Democr =     13.0 : 1.0\n",
      "                 defense = True           Republ : Democr =     13.0 : 1.0\n",
      "                    isis = True           Republ : Democr =     13.0 : 1.0\n",
      "                 liberal = True           Republ : Democr =     13.0 : 1.0\n",
      "                religion = True           Republ : Democr =     13.0 : 1.0\n",
      "                   trade = True           Republ : Democr =     12.7 : 1.0\n",
      "                    flag = True           Republ : Democr =     12.1 : 1.0\n",
      "               greatness = True           Republ : Democr =     12.1 : 1.0\n",
      "                 abraham = True           Republ : Democr =     11.9 : 1.0\n",
      "                  defund = True           Republ : Democr =     11.9 : 1.0\n",
      "                    drug = True           Republ : Democr =     10.9 : 1.0\n",
      "              department = True           Republ : Democr =     10.9 : 1.0\n",
      "               destroyed = True           Republ : Democr =     10.9 : 1.0\n",
      "                   enemy = True           Republ : Democr =     10.9 : 1.0\n",
      "               amendment = True           Republ : Democr =     10.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a little prose here about what you see in the classifier. Anything odd or interesting?\n",
    "\n",
    "### My Observations\n",
    "\n",
    "I found it very interesting that the most important features to the model were nearly all on the Republican side. Even going out to the 50 more important features, the only feature that favored Democrats was 'votes'. This is already a generic term that I'd expect both sides to use, so the model appears to be giving high weights to Republican features. This is likely the explanation to the low accuracy, as I think these words could be used in tweets by both sides. We'll further investigate these effects in the final section of the notebook.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Classifying Congressional Tweets\n",
    "\n",
    "In this part we apply the classifer we just built to a set of tweets by people running for congress\n",
    "in 2018. These tweets are stored in the database `congressional_data.db`. That DB is funky, so I'll\n",
    "give you the query I used to pull out the tweets. Note that this DB has some big tables and \n",
    "is unindexed, so the query takes a minute or two to run on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "cong_db = sqlite3.connect(\"congressional_data.db\")\n",
    "cong_cur = cong_db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cong_cur.execute(\n",
    "        '''\n",
    "           SELECT DISTINCT \n",
    "                  cd.candidate, \n",
    "                  cd.party,\n",
    "                  tw.tweet_text\n",
    "           FROM candidate_data cd \n",
    "           INNER JOIN tweets tw ON cd.twitter_handle = tw.handle \n",
    "               AND cd.candidate == tw.candidate \n",
    "               AND cd.district == tw.district\n",
    "           WHERE cd.party in ('Republican','Democratic') \n",
    "               AND tw.tweet_text NOT LIKE '%RT%'\n",
    "        ''')\n",
    "\n",
    "results = list(results) # Just to store it, since the query is time consuming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to go with the same puncation and stopword strategy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = []\n",
    "\n",
    "# Now fill up tweet_data with sublists like we did on the convention speeches.\n",
    "# Note that this may take a bit of time, since we have a lot of tweets.\n",
    "\n",
    "for row in results :\n",
    "\n",
    "    # Run the text variable through cleaning steps\n",
    "    text = row[2].decode('utf-8')\n",
    "    text_low = text.lower()\n",
    "    text_tok = tokenize(text_low)\n",
    "    text_clean = remove_punctuation(text_tok)\n",
    "    text_stop = remove_stop(text_clean)\n",
    "    text_untok = \" \".join(text_stop)\n",
    "\n",
    "    # I'll also remove urls\n",
    "    text_nourl = re.sub(r'http\\S+', '', text_untok).strip()\n",
    "    \n",
    "    # Store party as list\n",
    "    party = row[1]\n",
    "    \n",
    "    \n",
    "    # Create temp list\n",
    "    temp_list = [text_nourl, party]\n",
    "    \n",
    "    # Append to convention_data\n",
    "    tweet_data.append(temp_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of tweets here. Let's take a random sample and see how our classifer does. I'm guessing it won't be too great given the performance on the convention speeches..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20201014)\n",
    "\n",
    "tweet_data_sample = random.choices(tweet_data,k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's our (cleaned) tweet: earlier today spoke house floor abt protecting health care women praised ppmarmonte work central coast\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: go tribe #rallytogether\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: apparently trump thinks easy students overwhelmed crushing burden debt pay student loans #trumpbudget\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: we’re grateful first responders rescue personnel firefighters police volunteers working tirelessly keep people safe provide muchneeded help putting lives line\n",
      "Actual party is Republican and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: let’s make even greater  #kag 🇺🇸\n",
      "Actual party is Republican and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: 1hr cavs tie series 22 i'm #allin216 repbarbaralee scared #roadtovictory\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: congrats belliottsd new gig sd city hall glad continue serve…\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: really close 3500 raised toward match right whoot that’s 7000 nonmath majors room 😂 help us get\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: today comment period potus’s plan expand offshore drilling opened public 60 days march 9 share oppose proposed program directly trump administration comments made email mail\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: celebrated icseastla’s 22 years eastside commitment amp saluted community leaders last night’s awards dinner\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for tweet, party in tweet_data_sample :\n",
    "    # Convert text to our feature word dictionary\n",
    "    features = conv_features(text=tweet)\n",
    "\n",
    "    # Run our classifier\n",
    "    estimated_party = classifier.classify(featureset=features)\n",
    "    # Fill in the right-hand side above with code that estimates the actual party\n",
    "    \n",
    "    print(f\"Here's our (cleaned) tweet: {tweet}\")\n",
    "    print(f\"Actual party is {party} and our classifer says {estimated_party}.\")\n",
    "    print(\"\")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've looked at it some, let's score a bunch and see how we're doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of counts by actual party and estimated party. \n",
    "# first key is actual, second is estimated\n",
    "parties = ['Republican','Democratic']\n",
    "results = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for p in parties :\n",
    "    for p1 in parties :\n",
    "        results[p][p1] = 0\n",
    "\n",
    "\n",
    "num_to_score = 10000\n",
    "random.shuffle(tweet_data)\n",
    "\n",
    "for idx, tp in enumerate(tweet_data) :\n",
    "    tweet, party = tp    \n",
    "    # Now do the same thing as above, but we store the results rather\n",
    "    # than printing them. \n",
    "  \n",
    "    # Convert text to our feature word dictionary\n",
    "    features = conv_features(text=tweet)\n",
    "\n",
    "    # Run our classifier\n",
    "    estimated_party = classifier.classify(featureset=features)\n",
    "\n",
    "    results[party][estimated_party] += 1\n",
    "    \n",
    "    if idx > num_to_score : \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'Republican': defaultdict(int,\n",
       "                         {'Republican': 3694, 'Democratic': 678}),\n",
       "             'Democratic': defaultdict(int,\n",
       "                         {'Republican': 4727, 'Democratic': 903})})"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classifier predicted 84.49 percent of actual Republican tweets correctly\n",
      "The classifier predicted 16.04 percent of actual Democratic tweets correctly\n",
      "The classifier predicted 45.97 percent of all the tweets correctly\n"
     ]
    }
   ],
   "source": [
    "# Actual Republicans\n",
    "rep_correct = results.get('Republican').get('Republican')\n",
    "rep_total = results.get('Republican').get('Democratic') + rep_correct\n",
    "rep_acc = rep_correct/rep_total*100\n",
    "print(\"The classifier predicted\", round(rep_acc, 2), \"percent of actual Republican tweets correctly\")\n",
    "\n",
    "# Actual Democrats\n",
    "dem_correct = results.get('Democratic').get('Democratic')\n",
    "dem_total = results.get('Democratic').get('Republican') + dem_correct\n",
    "dem_acc = dem_correct/dem_total*100\n",
    "print(\"The classifier predicted\", round(dem_acc, 2), \"percent of actual Democratic tweets correctly\")\n",
    "\n",
    "# Total\n",
    "total_acc = (dem_correct + rep_correct)/num_to_score * 100\n",
    "print(\"The classifier predicted\", round(total_acc, 2), \"percent of all the tweets correctly\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting? We'll take a look at the accuracy on the convention train and test sets to see if we're overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.54\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(round(nltk.classify.accuracy(classifier, train_set), 2))\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the samples unbalanced? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Democratic': 1243, 'Republican': 798})"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parties = [x[1] for x in train_set]\n",
    "collections.Counter(parties)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflections\n",
    "\n",
    "The results of both of the models paint a pretty clear picture. They were both pretty poor, with the convention data only having a 50% accuracy and the twitter data only ending up with a 46% accuracy. From the results dictionary, we saw that the model predicted 'Republican' for nearly all of the tweets, even for actual Democrats. So, at first I investigated if the model was potentially overfit by comparing the train and test accuracy of our convention data (since we used it to train the data). Our test accuracy was only 50%, not much better than the training accuracy. I then investigated if maybe there was a sample imbalance of our convention data, thinking maybe the classifier was optimizing accuracy and our training set was made of nearly all Republican speeches. However, our training data actually had more Democrat texts. Finally, I thought there were plenty of features in our feature words to capture everything. \n",
    "\n",
    "So, my suggestion to improve the classifier would turn to a different model beside Naive Bayes. Likely a non-linear model to help us capture a little bit more of the variance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
